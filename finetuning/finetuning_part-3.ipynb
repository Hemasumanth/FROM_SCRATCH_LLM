{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import supplementary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import json\n",
    "import tiktoken\n",
    "import supplementary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_token_ids(text, tokenizer):\n",
    "    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n",
    "    encoded_tensor = torch.tensor(encoded).unsqueeze(0)  # add batch dimension\n",
    "    return encoded_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_ids_to_text(token_ids, tokenizer):\n",
    "    flat = token_ids.squeeze(0)  # remove batch dimension\n",
    "    return tokenizer.decode(flat.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FineTuningDataset(Dataset):\n",
    "    def __init__(self, json_file, tokenizer, max_length=128):\n",
    "        # Load JSON data from file\n",
    "        with open(json_file, 'r') as f:\n",
    "            self.data = json.load(f)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        instruction = item[\"instruction\"]\n",
    "        input_text = item[\"input\"]\n",
    "        output_text = item[\"output\"]\n",
    "\n",
    "        # Combine instruction, input, and output with <|endoftext|> as separator\n",
    "        full_text = f\"{instruction} Input: {input_text} Output: {output_text}<|endoftext|>\"\n",
    "\n",
    "        # Convert to token IDs\n",
    "        token_ids = text_to_token_ids(full_text, self.tokenizer).squeeze(0)  # Remove batch dim for now\n",
    "\n",
    "        # Truncate or pad to max_length\n",
    "        if token_ids.size(0) > self.max_length:\n",
    "            token_ids = token_ids[:self.max_length]\n",
    "        else:\n",
    "            padding = torch.zeros(self.max_length - token_ids.size(0), dtype=torch.long)\n",
    "            token_ids = torch.cat([token_ids, padding])\n",
    "\n",
    "        return token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_file = \"C:/Users/suman/OneDrive/Desktop/From_Scratch_LLM/finetuning/instruction-data.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\suman\\AppData\\Local\\Temp\\ipykernel_69448\\2725126064.py:12: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  gpt = torch.load(model_path, map_location=device)\n"
     ]
    }
   ],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "tokenizer.pad_token_id = tokenizer.encode(\"<|endoftext|>\", allowed_special={'<|endoftext|>'})[0]  # 50256\n",
    "tokenizer.vocab_size = tokenizer.n_vocab  # 50257\n",
    "\n",
    "# 5. Create dataset and dataloader\n",
    "dataset = FineTuningDataset(json_file, tokenizer, max_length=128)\n",
    "dataloader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
    "\n",
    "# 6. Load the pre-trained model\n",
    "model_path = r\"C:\\Users\\suman\\OneDrive\\Desktop\\From_Scratch_LLM\\weightloading\\pretrained_gpt_full_model.pth\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "gpt = torch.load(model_path, map_location=device)\n",
    "gpt.to(device)\n",
    "\n",
    "# 7. Set up optimizer and loss function\n",
    "optimizer = torch.optim.AdamW(gpt.parameters(), lr=1e-4)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloader, tokenizer, epochs=3):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for batch in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Move batch to device\n",
    "            input_ids = batch.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            logits = model(input_ids)\n",
    "\n",
    "            # Shift input_ids and logits for language modeling\n",
    "            shift_logits = logits[..., :-1, :].contiguous()\n",
    "            shift_labels = input_ids[..., 1:].contiguous()\n",
    "\n",
    "            # Compute loss\n",
    "            loss = criterion(shift_logits.view(-1, cfg[\"vocab_size\"]), shift_labels.view(-1))\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}, Average Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    # Example inference after training\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        sample_input = \"Evaluate the following phrase by transforming it into the spelling given. Input: freind --> friend Output:\"\n",
    "        token_ids = text_to_token_ids(sample_input, tokenizer).to(device)\n",
    "        output_logits = model(token_ids)\n",
    "        predicted_ids = torch.argmax(output_logits, dim=-1)\n",
    "        print(\"Sample Output:\", token_ids_to_text(predicted_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,   # Vocabulary size\n",
    "    \"context_length\": 256, # Shortened context length (orig: 1024)\n",
    "    \"emb_dim\": 768,        # Embedding dimension\n",
    "    \"n_heads\": 12,         # Number of attention heads\n",
    "    \"n_layers\": 12,        # Number of layers\n",
    "    \"drop_rate\": 0.1,      # Dropout rate\n",
    "    \"qkv_bias\": False      # Query-key-value bias\n",
    "}\n",
    "\n",
    "\n",
    "# Define model configurations in a dictionary for compactness\n",
    "model_configs = {\n",
    "    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
    "    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
    "    \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
    "    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n",
    "}\n",
    "\n",
    "# Copy the base configuration and update with specific model settings\n",
    "model_name = \"gpt2-small (124M)\"  # Example model name\n",
    "NEW_CONFIG = GPT_CONFIG_124M.copy()\n",
    "NEW_CONFIG.update(model_configs[model_name])\n",
    "NEW_CONFIG.update({\"context_length\": 1024, \"qkv_bias\": True})\n",
    "\n",
    "cfg=NEW_CONFIG\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model(gpt, dataloader, tokenizer, epochs=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. Save the fine-tuned model (optional)\n",
    "torch.save(gpt.state_dict(), \"fine_tuned_gpt.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt = GPTModel(cfg)\n",
    "\n",
    "state_dict_path = \"fine_tuned_gpt.pth\"  \n",
    "# state_dict_path = r\"C:\\Users\\suman\\OneDrive\\Desktop\\From_Scratch_LLM\\weightloading\\fine_tuned_gpt.pth\"\n",
    "\n",
    "# Load the state_dict\n",
    "state_dict = torch.load(state_dict_path, map_location=\"cpu\")  # Load to CPU first\n",
    "gpt.load_state_dict(state_dict)\n",
    "\n",
    "# Move to the desired device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "gpt.to(device)\n",
    "\n",
    "# Set to evaluation mode (optional, for inference)\n",
    "gpt.eval()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLMs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
